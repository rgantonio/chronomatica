{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6461a1-207c-4b56-8b69-27c95dbed0c4",
   "metadata": {},
   "source": [
    "# ResNet20 Study\n",
    "- This notebook simply downloads and breaks down a ResNet20 model\n",
    "- The purpose of this study is just to see how it can be used\n",
    "- The model can be best seen in this link: https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d7116-c5fd-4e25-980c-9930382dd5c8",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c6bb42c-1e2d-49aa-94b0-8edb5abdceab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:11:44.430485Z",
     "iopub.status.busy": "2025-05-22T06:11:44.428863Z",
     "iopub.status.idle": "2025-05-22T06:11:51.190548Z",
     "shell.execute_reply": "2025-05-22T06:11:51.189534Z",
     "shell.execute_reply.started": "2025-05-22T06:11:44.430407Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d947ec-267e-4ee1-b36a-2bdf731e4191",
   "metadata": {},
   "source": [
    "## Downloading the model\n",
    "- The override command is because github limits the download rate. This overrides it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ffe5c4-d3c9-41cd-abbb-41c6d7f300da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:11:51.192006Z",
     "iopub.status.busy": "2025-05-22T06:11:51.191705Z",
     "iopub.status.idle": "2025-05-22T06:11:51.531787Z",
     "shell.execute_reply": "2025-05-22T06:11:51.530828Z",
     "shell.execute_reply.started": "2025-05-22T06:11:51.191983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /users/micas/rantonio/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "# Override the validation function to bypass the rate limit error\n",
    "torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n",
    "\n",
    "# Load the pretrained ResNet-20 model for CIFAR-10\n",
    "model = torch.hub.load('chenyaofo/pytorch-cifar-models', 'cifar10_resnet20', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2c629-2f53-415c-90d2-b3d4dadb9df3",
   "metadata": {},
   "source": [
    "## Normalization of Input Data\n",
    "- Note, that these normalize values come from the training set of CIFAR10\n",
    "- This normalization is a necessity since the CIFAR10 was trained with the normalization in place\n",
    "- The first set of 3 tuples is for the mean, while the 2nd is for the std. dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972ed95e-a087-4202-a696-6b80e9e3e971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:11:51.708671Z",
     "iopub.status.busy": "2025-05-22T06:11:51.707942Z",
     "iopub.status.idle": "2025-05-22T06:11:51.716678Z",
     "shell.execute_reply": "2025-05-22T06:11:51.715683Z",
     "shell.execute_reply.started": "2025-05-22T06:11:51.708607Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089941e-cdc8-4a41-9d18-6602eb969c0b",
   "metadata": {},
   "source": [
    "## Downloading CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a8be2b-d441-49cb-9d9f-31be3f61bcda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:11:57.333977Z",
     "iopub.status.busy": "2025-05-22T06:11:57.333425Z",
     "iopub.status.idle": "2025-05-22T06:11:58.073642Z",
     "shell.execute_reply": "2025-05-22T06:11:58.072233Z",
     "shell.execute_reply.started": "2025-05-22T06:11:57.333928Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56602f-e2f8-483b-8ca2-e3734a5e688d",
   "metadata": {},
   "source": [
    "## Set Model for Evaluation\n",
    "- You can also see here the details of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c10a0cd-1c99-49bd-b729-f269744f3bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:12:05.768420Z",
     "iopub.status.busy": "2025-05-22T06:12:05.766372Z",
     "iopub.status.idle": "2025-05-22T06:12:05.783367Z",
     "shell.execute_reply": "2025-05-22T06:12:05.782096Z",
     "shell.execute_reply.started": "2025-05-22T06:12:05.768348Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CifarResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33e8f4-4d2f-410c-a75c-edcfef4169af",
   "metadata": {},
   "source": [
    "- The code below shows the model better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aa3cbb7-430a-4fe5-9ef0-ae55592c55e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:31:31.842796Z",
     "iopub.status.busy": "2025-05-22T06:31:31.841753Z",
     "iopub.status.idle": "2025-05-22T06:31:31.853730Z",
     "shell.execute_reply": "2025-05-22T06:31:31.851958Z",
     "shell.execute_reply.started": "2025-05-22T06:31:31.842710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "layer1\n",
      "layer1.0\n",
      "layer1.0.conv1\n",
      "layer1.0.bn1\n",
      "layer1.0.relu\n",
      "layer1.0.conv2\n",
      "layer1.0.bn2\n",
      "layer1.1\n",
      "layer1.1.conv1\n",
      "layer1.1.bn1\n",
      "layer1.1.relu\n",
      "layer1.1.conv2\n",
      "layer1.1.bn2\n",
      "layer1.2\n",
      "layer1.2.conv1\n",
      "layer1.2.bn1\n",
      "layer1.2.relu\n",
      "layer1.2.conv2\n",
      "layer1.2.bn2\n",
      "layer2\n",
      "layer2.0\n",
      "layer2.0.conv1\n",
      "layer2.0.bn1\n",
      "layer2.0.relu\n",
      "layer2.0.conv2\n",
      "layer2.0.bn2\n",
      "layer2.0.downsample\n",
      "layer2.0.downsample.0\n",
      "layer2.0.downsample.1\n",
      "layer2.1\n",
      "layer2.1.conv1\n",
      "layer2.1.bn1\n",
      "layer2.1.relu\n",
      "layer2.1.conv2\n",
      "layer2.1.bn2\n",
      "layer2.2\n",
      "layer2.2.conv1\n",
      "layer2.2.bn1\n",
      "layer2.2.relu\n",
      "layer2.2.conv2\n",
      "layer2.2.bn2\n",
      "layer3\n",
      "layer3.0\n",
      "layer3.0.conv1\n",
      "layer3.0.bn1\n",
      "layer3.0.relu\n",
      "layer3.0.conv2\n",
      "layer3.0.bn2\n",
      "layer3.0.downsample\n",
      "layer3.0.downsample.0\n",
      "layer3.0.downsample.1\n",
      "layer3.1\n",
      "layer3.1.conv1\n",
      "layer3.1.bn1\n",
      "layer3.1.relu\n",
      "layer3.1.conv2\n",
      "layer3.1.bn2\n",
      "layer3.2\n",
      "layer3.2.conv1\n",
      "layer3.2.bn1\n",
      "layer3.2.relu\n",
      "layer3.2.conv2\n",
      "layer3.2.bn2\n",
      "avgpool\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "   print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c40f2-51cf-4fbb-94eb-d1fc8337c7e9",
   "metadata": {},
   "source": [
    "## Running the model to check for accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82369c6e-ad94-461d-a102-a0b7d4ba2389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:12:10.230125Z",
     "iopub.status.busy": "2025-05-22T06:12:10.228659Z",
     "iopub.status.idle": "2025-05-22T06:12:14.970640Z",
     "shell.execute_reply": "2025-05-22T06:12:14.969907Z",
     "shell.execute_reply.started": "2025-05-22T06:12:10.230057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.60%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a9f18-4816-4b50-9ca5-fb97fbe05363",
   "metadata": {},
   "source": [
    "# Model Details\n",
    "\n",
    "The general structure of the ResNet20 is as follows:\n",
    "\n",
    "1. **Initial Convolution**:\n",
    "\n",
    "- Conv2D: 16 filters, 3×3 kernel, stride 1, padding 1\n",
    "\n",
    "2. **Residual Blocks**:\n",
    "\n",
    "- ResNet-20 is structured with 3 stages, each containing 3 residual blocks (totaling 9 blocks). Each block consists of two 3×3 convolutional layers.\n",
    "\n",
    "- **Stage 1**:\n",
    "  - 3 Residual Blocks, each with:\n",
    "    - Conv2D: 16 filters, 3×3 kernel, stride 1, padding 1\n",
    "    - BatchNorm + ReLU\n",
    "    - Conv2D: 16 filters, 3×3 kernel, stride 1, padding 1\n",
    "    - BatchNorm\n",
    "    - Skip connection (identity)\n",
    "    - ReLU\n",
    "\n",
    "- **Stage 2**:\n",
    "  - 3 Residual Blocks, each with:\n",
    "    - Conv2D: 32 filters, 3×3 kernel, stride 2 (first block), stride 1 (others), padding 1\n",
    "    - BatchNorm + ReLU\n",
    "    - Conv2D: 32 filters, 3×3 kernel, stride 1, padding 1\n",
    "    - BatchNorm\n",
    "    - Skip connection (with 1×1 convolution for dimension matching in first block)\n",
    "    - ReLU\n",
    "\n",
    "- **Stage 3**:\n",
    "  - 3 Residual Blocks, each with:\n",
    "    - Conv2D: 64 filters, 3×3 kernel, stride 2 (first block), stride 1 (others), padding 1\n",
    "    - BatchNorm + ReLU\n",
    "    - Conv2D: 64 filters, 3×3 kernel, stride 1, padding 1\n",
    "    - BatchNorm\n",
    "    - Skip connection (with 1×1 convolution for dimension matching in first block)\n",
    "    - ReLU\n",
    "\n",
    "3. **Final Layers**:\n",
    "  - Global Average Pooling: Reduces each feature map to a single value (1×1×64)\n",
    "  - Fully Connected Layer: 10 outputs (for CIFAR-10 classes)\n",
    "  - Softmax Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd965622-d3cc-4b74-adf9-90b335f00aa6",
   "metadata": {},
   "source": [
    "## Input Size\n",
    "- The model description can be seen from the `model.eval()`\n",
    "- Note that the input is of size `[3,32,32]` $\\rightarrow$ `channels, height, width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "468b93e1-6a87-41df-a6cc-2c65286701b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:12:15.661982Z",
     "iopub.status.busy": "2025-05-22T06:12:15.661219Z",
     "iopub.status.idle": "2025-05-22T06:12:15.672799Z",
     "shell.execute_reply": "2025-05-22T06:12:15.670965Z",
     "shell.execute_reply.started": "2025-05-22T06:12:15.661918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "img, label = test_dataset[214]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d557e-b4e2-45b0-86e2-ac38dd0fbe3a",
   "metadata": {},
   "source": [
    "- We can display the image but we first we convert the shape to (H,W,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e8bbcfe-3a13-43a9-92b2-9cd0f5e783fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:12:20.883193Z",
     "iopub.status.busy": "2025-05-22T06:12:20.882443Z",
     "iopub.status.idle": "2025-05-22T06:12:20.891812Z",
     "shell.execute_reply": "2025-05-22T06:12:20.889928Z",
     "shell.execute_reply.started": "2025-05-22T06:12:20.883130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert tensor to NumPy and permute dimensions from (C, H, W) to (H, W, C)\n",
    "img = img.permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc016a7-a9b3-4213-8d22-6e46cec6752f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:12:21.231975Z",
     "iopub.status.busy": "2025-05-22T06:12:21.230425Z",
     "iopub.status.idle": "2025-05-22T06:12:21.354290Z",
     "shell.execute_reply": "2025-05-22T06:12:21.353213Z",
     "shell.execute_reply.started": "2025-05-22T06:12:21.231905Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3002539..2.4415667].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40361b45-9894-4be6-b1e1-f8eeb2b0c7c8",
   "metadata": {},
   "source": [
    "## Details of Convolution Layers\n",
    "- We can dump the weights and the shape of all the convolution layers\n",
    "- You can also see the weight shapes of all convolution layers\n",
    "- The output shapes are of `[out_ch, in_ch, kernel_h, kernel_w]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c03035-11ce-4aea-8a2b-41c7342b3002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:13:23.524961Z",
     "iopub.status.busy": "2025-05-22T06:13:23.524129Z",
     "iopub.status.idle": "2025-05-22T06:13:23.536411Z",
     "shell.execute_reply": "2025-05-22T06:13:23.534436Z",
     "shell.execute_reply.started": "2025-05-22T06:13:23.524898Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: torch.Size([16, 3, 3, 3])\n",
      "layer1.0.conv1: torch.Size([16, 16, 3, 3])\n",
      "layer1.0.conv2: torch.Size([16, 16, 3, 3])\n",
      "layer1.1.conv1: torch.Size([16, 16, 3, 3])\n",
      "layer1.1.conv2: torch.Size([16, 16, 3, 3])\n",
      "layer1.2.conv1: torch.Size([16, 16, 3, 3])\n",
      "layer1.2.conv2: torch.Size([16, 16, 3, 3])\n",
      "layer2.0.conv1: torch.Size([32, 16, 3, 3])\n",
      "layer2.0.conv2: torch.Size([32, 32, 3, 3])\n",
      "layer2.0.downsample.0: torch.Size([32, 16, 1, 1])\n",
      "layer2.1.conv1: torch.Size([32, 32, 3, 3])\n",
      "layer2.1.conv2: torch.Size([32, 32, 3, 3])\n",
      "layer2.2.conv1: torch.Size([32, 32, 3, 3])\n",
      "layer2.2.conv2: torch.Size([32, 32, 3, 3])\n",
      "layer3.0.conv1: torch.Size([64, 32, 3, 3])\n",
      "layer3.0.conv2: torch.Size([64, 64, 3, 3])\n",
      "layer3.0.downsample.0: torch.Size([64, 32, 1, 1])\n",
      "layer3.1.conv1: torch.Size([64, 64, 3, 3])\n",
      "layer3.1.conv2: torch.Size([64, 64, 3, 3])\n",
      "layer3.2.conv1: torch.Size([64, 64, 3, 3])\n",
      "layer3.2.conv2: torch.Size([64, 64, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        print(f\"{name}: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a3941-2e23-4359-9a83-e6793bd12076",
   "metadata": {},
   "source": [
    "- The code below is to extract the output sizes\n",
    "- The important part is to see how the output sizes change as we go deeper into the layers\n",
    "  - That is the H,W get smaller but the C gets larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de4c99e-0ff4-46bb-805f-5ed50f9ff436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:14:17.348694Z",
     "iopub.status.busy": "2025-05-22T06:14:17.347916Z",
     "iopub.status.idle": "2025-05-22T06:14:17.485406Z",
     "shell.execute_reply": "2025-05-22T06:14:17.484532Z",
     "shell.execute_reply.started": "2025-05-22T06:14:17.348631Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: output shape torch.Size([1, 16, 32, 32])\n",
      "bn1: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.0.conv1: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.0.bn1: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.0.conv2: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.0.bn2: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.1.conv1: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.1.bn1: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.1.conv2: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.1.bn2: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.2.conv1: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.2.bn1: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.2.conv2: output shape torch.Size([1, 16, 32, 32])\n",
      "layer1.2.bn2: output shape torch.Size([1, 16, 32, 32])\n",
      "layer2.0.conv1: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.0.bn1: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.0.conv2: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.0.bn2: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.0.downsample.0: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.0.downsample.1: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.1.conv1: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.1.bn1: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.1.conv2: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.1.bn2: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.2.conv1: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.2.bn1: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.2.conv2: output shape torch.Size([1, 32, 16, 16])\n",
      "layer2.2.bn2: output shape torch.Size([1, 32, 16, 16])\n",
      "layer3.0.conv1: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.0.bn1: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.0.conv2: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.0.bn2: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.0.downsample.0: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.0.downsample.1: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.1.conv1: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.1.bn1: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.1.conv2: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.1.bn2: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.2.conv1: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.2.bn1: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.2.conv2: output shape torch.Size([1, 64, 8, 8])\n",
      "layer3.2.bn2: output shape torch.Size([1, 64, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "def hook_fn(module, input, output):\n",
    "    # The module object doesn't have the name directly here,\n",
    "    # so we capture name via a closure below\n",
    "    print(f\"{module_name}: output shape {output.shape}\")\n",
    "\n",
    "# To pass names, register hooks with closures:\n",
    "hooks = []\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, (torch.nn.Conv2d, torch.nn.BatchNorm2d)):\n",
    "        module_name = name  # Capture name in closure\n",
    "\n",
    "        # Use a lambda to keep the name bound correctly:\n",
    "        hooks.append(layer.register_forward_hook(\n",
    "            lambda module, input, output, module_name=module_name: \n",
    "                print(f\"{module_name}: output shape {output.shape}\")\n",
    "        ))\n",
    "\n",
    "# Dummy input for forward pass\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "with torch.no_grad():\n",
    "    model(dummy_input)\n",
    "\n",
    "# Remove hooks after use (good practice)\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15674ec-546d-4946-89e2-3a8d85788f23",
   "metadata": {},
   "source": [
    "## Details of BatchNorm Layers\n",
    "- This is to get the batchnorm's tuning parameters as well.\n",
    "- Take note that the batchnorm is applied channel-wise that's why you see 16, 32, and 64 elements for gamma and beta and the mean and variance normalization\n",
    "- Equation wise we have:\n",
    "$$ \\hat{x}_c = \\frac{x_c - \\mu_c}{\\sqrt{\\sigma^2_c + \\epsilon}}$$\n",
    "\n",
    "$$ y_c = \\gamma_c \\cdot \\hat{x}_c + \\beta_c $$\n",
    "- Note that after batchnorm the output shapes don't change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e36d1b1-ade4-408d-bc91-97c1a76d7359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:14:22.831377Z",
     "iopub.status.busy": "2025-05-22T06:14:22.829861Z",
     "iopub.status.idle": "2025-05-22T06:14:22.841606Z",
     "shell.execute_reply": "2025-05-22T06:14:22.839737Z",
     "shell.execute_reply.started": "2025-05-22T06:14:22.831300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn1: gamma=torch.Size([16]), beta=torch.Size([16])\n",
      "layer1.0.bn1: gamma=torch.Size([16]), beta=torch.Size([16])\n",
      "layer1.0.bn2: gamma=torch.Size([16]), beta=torch.Size([16])\n",
      "layer1.1.bn1: gamma=torch.Size([16]), beta=torch.Size([16])\n",
      "layer1.1.bn2: gamma=torch.Size([16]), beta=torch.Size([16])\n",
      "layer1.2.bn1: gamma=torch.Size([16]), beta=torch.Size([16])\n",
      "layer1.2.bn2: gamma=torch.Size([16]), beta=torch.Size([16])\n",
      "layer2.0.bn1: gamma=torch.Size([32]), beta=torch.Size([32])\n",
      "layer2.0.bn2: gamma=torch.Size([32]), beta=torch.Size([32])\n",
      "layer2.0.downsample.1: gamma=torch.Size([32]), beta=torch.Size([32])\n",
      "layer2.1.bn1: gamma=torch.Size([32]), beta=torch.Size([32])\n",
      "layer2.1.bn2: gamma=torch.Size([32]), beta=torch.Size([32])\n",
      "layer2.2.bn1: gamma=torch.Size([32]), beta=torch.Size([32])\n",
      "layer2.2.bn2: gamma=torch.Size([32]), beta=torch.Size([32])\n",
      "layer3.0.bn1: gamma=torch.Size([64]), beta=torch.Size([64])\n",
      "layer3.0.bn2: gamma=torch.Size([64]), beta=torch.Size([64])\n",
      "layer3.0.downsample.1: gamma=torch.Size([64]), beta=torch.Size([64])\n",
      "layer3.1.bn1: gamma=torch.Size([64]), beta=torch.Size([64])\n",
      "layer3.1.bn2: gamma=torch.Size([64]), beta=torch.Size([64])\n",
      "layer3.2.bn1: gamma=torch.Size([64]), beta=torch.Size([64])\n",
      "layer3.2.bn2: gamma=torch.Size([64]), beta=torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        print(f\"{name}: gamma={module.weight.shape}, beta={module.bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035a3bb-5c4b-4461-8eb7-366438767f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this to get the weight parameters\n",
    "# No need to run for the sake of documentation in github\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        gamma = module.weight.detach().cpu().numpy()  # scaling factors\n",
    "        beta = module.bias.detach().cpu().numpy()     # shifting factors\n",
    "        running_mean = module.running_mean.detach().cpu().numpy()  # BN running mean\n",
    "        running_var = module.running_var.detach().cpu().numpy()    # BN running variance\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  gamma (weight): {gamma}\")\n",
    "        print(f\"  beta (bias): {beta}\")\n",
    "        print(f\"  running_mean: {running_mean}\")\n",
    "        print(f\"  running_var: {running_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92169690-39da-401a-bbdf-fbdc9a5791da",
   "metadata": {},
   "source": [
    "## Final layer\n",
    "\n",
    "- The code below extracts the details of the avgpool and FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e38c2c74-9045-474a-bf3e-90259683efe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T06:49:38.900338Z",
     "iopub.status.busy": "2025-05-22T06:49:38.898444Z",
     "iopub.status.idle": "2025-05-22T06:49:38.913443Z",
     "shell.execute_reply": "2025-05-22T06:49:38.912170Z",
     "shell.execute_reply.started": "2025-05-22T06:49:38.900258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avgpool detail: AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "FC layer weight shape: torch.Size([10, 64])\n",
      "FC layer bias shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "avgpool_layer = model.avgpool\n",
    "fc_layer = model.fc  # In this model, it's named 'linear'\n",
    "\n",
    "print(\"Avgpool detail:\", avgpool_layer)\n",
    "print(\"FC layer weight shape:\", fc_layer.weight.shape)\n",
    "print(\"FC layer bias shape:\", fc_layer.bias.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
